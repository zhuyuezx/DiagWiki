# ============================================================
# DiagWiki Configuration
# ============================================================
# Copy this file to .env and customize the values for your setup

# ============================================================
# Application Settings
# ============================================================
NODE_ENV=[development|production]
PORT=[backend server port, default: 8001]
LOG_LEVEL=[logging level: DEBUG|INFO|WARNING|ERROR]

# ============================================================
# LLM Configuration
# ============================================================
# Ollama server URL
OLLAMA_HOST=[ollama server url, usually http://localhost:11434]

# LLM API timeout in seconds (prevents indefinite hangs)
# Set higher if using large models that take time to load
LLM_TIMEOUT=[timeout in seconds, recommended: 120.0 for 30B models]

# How long to keep models loaded in memory after last request
# Prevents frequent reloads (10m = 10 minutes)
OLLAMA_KEEP_ALIVE=[keep-alive duration, e.g., 10m for 10 minutes]

# Model names (ensure these models are available in your Ollama server)
GENERATION_MODEL=[ollama model for code generation, e.g., qwen3-coder:30b]
EMBEDDING_MODEL=[ollama embedding model, e.g., nomic-embed-text]

# Embedding API timeout (embeddings are typically faster)
EMBEDDING_TIMEOUT=[timeout in seconds, recommended: 30.0]

# ============================================================
# Text Splitting Configuration
# ============================================================
# How to split documents: "token", "word", or "sentence"
TEXT_SPLIT_BY=[split method: token|word|sentence]

# Size of each chunk in tokens/words
TEXT_CHUNK_SIZE=[chunk size, recommended: 1000]

# Overlap between chunks to maintain context
TEXT_CHUNK_OVERLAP=[overlap size, recommended: 50]

# ============================================================
# Localization
# ============================================================
# Default language for wiki generation
# Supported: en (English), ja (Japanese), zh (Chinese), es (Spanish), kr (Korean), vi (Vietnamese), etc.
DEFAULT_LANGUAGE=[language code, default: en]

# ============================================================
# RAG (Retrieval Augmented Generation) Configuration
# ============================================================
# Maximum characters for RAG context (prevents LLM overflow)
MAX_RAG_CONTEXT_CHARS=[max context chars, recommended: 100000]

# Maximum number of source files to include
MAX_SOURCES=[max source files, recommended: 40]

# Maximum characters per file when reading manual references
MAX_FILE_CHARS=[max chars per file, recommended: 50000]

# Default number of chunks to retrieve
RAG_TOP_K=[number of chunks to retrieve, recommended: 40]

# Higher top_k for section identification (more comprehensive view)
RAG_SECTION_ITERATION_TOP_K=[higher top_k for iterations, recommended: 80]

# Maximum tokens for document chunking
MAX_TOKEN_LIMIT=[token limit for documents, recommended: 8192]

# Maximum tokens for embedding (to prevent overflow)
MAX_EMBEDDING_TOKENS=[max embedding tokens, recommended: 6000]

# Characters to show in source previews
SOURCE_PREVIEW_LENGTH=[preview length in chars, recommended: 600]

# ============================================================
# LLM Generation Parameters
# ============================================================
# Temperature for creative generation (0.0-1.0)
# Higher = more creative, Lower = more focused
DEFAULT_TEMPERATURE=[temperature for generation, range: 0.0-1.0, recommended: 0.7]

# Temperature for focused tasks like title generation
FOCUSED_TEMPERATURE=[temperature for focused tasks, range: 0.0-1.0, recommended: 0.3]

# Context window size for LLM
LARGE_CONTEXT_WINDOW=[context window size, recommended: 16384]

# ============================================================
# API Configuration
# ============================================================
# Thread pool size for concurrent operations
MAX_WORKERS=[number of worker threads, recommended: 4]
